#!/bin/bash


PURPOSE=baseline
JUDGECLASS="oldreut"

#CORPLIST=("robust04_0" "robust04_1" "robust04_2" "robust04_3" "robust04_4" "robust04_5")
#CORPLIST=("FBIS" "FT" "FR" "LA")
CORPLIST=("oldreut")
SOFIA="/home/guilherme/Downloads/sofia-ml/trunk/sofia-ml/sofia-ml"

for CORP in "${CORPLIST[@]}"
   do
      # if ! [ -e Corpus/"$CORP".tgz ] ; then
      # tar -cvzf Corpus/"$CORP".tgz Corpus/"$CORP"/
      # fi 
      
   
      pushd Corpus
      
      if [ ! -e "$CORP".svm.fil ] || [ ! -e "$CORP".df ]; then
        ./dofast4 "$CORP"
      fi
      
      cp "$CORP".df ../"$CORP".df
      
      cp "$CORP".svm.fil ../"$CORP".svm.fil
      
      popd

      while IFS='' read -r line || [[ -n $line ]]; do
         IFS=':' read -ra TEXT <<< "$line"

         TOPIC="${TEXT[0]}"
         QUERY="${TEXT[1]}"
         echo "$TOPIC"
         echo "$QUERY"

         
         
         rm -rf result/"$PURPOSE"/"$CORP"/"$TOPIC"/
         mkdir -p result/"$PURPOSE"/"$CORP"/
         mkdir -p result/dump/"$PURPOSE"/"$CORP"/
         
         rm -rf $TOPIC
          mkdir $TOPIC
 
 
          echo `wc -l < "$CORP".svm.fil` > N
          pushd $TOPIC 
# 
          echo "$QUERY" > "$TOPIC".seed.doc
# 
#        
# 
# 
         cut -d' ' -f1 ../$CORP.svm.fil | sed -e 's/.*/& &/' > docfil
         cut -d' ' -f1 docfil | cat -n > docfils
#          
# 
# 
         touch rel.$TOPIC.fil
         
    #       cut -f2 docfil | join - $TOPIC.seed.sorted | cut -d' ' -f2 >> rel.$TOPIC.fil

         touch prel.$TOPIC
         rm -rf prevalence.rate
         touch prevalence.rate
         rm -rf rel.rate
         touch rel.rate


         rm -f new[0-9][0-9].$TOPIC tail[0-9][0-9].$TOPIC self*.$TOPIC gold*.$TOPIC
         touch new00.$TOPIC
# 
# 
         NDOCS=`cat docfils | wc -l`
         NDUN=0
         L=1
         R=100
         Rel=0
         B=0
         export LAMBDA=0.0001

         cp $TOPIC.seed.doc ../$TOPIC.seed.doc
         popd
# 
          ./dofeaturesseed4 $TOPIC.seed.doc $TOPIC $CORP
          pushd $TOPIC
          sed -e 's/[^ ]*/0/' ../$CORP.svm.fil | ../dosplit
          sed -e 's/[^ ]*/1/' svm.$TOPIC.seed.doc.fil > $TOPIC.synthetic.seed


         for x in 0 1 2 3 4 5 6 7 8 9 ; do
            for y in 0 1 2 3 4 5 6 7 8 9 ; do
            if [ $NDUN -lt $NDOCS ] ; then
               export N=$x$y
               cp $TOPIC.synthetic.seed trainset
               
               echo "starting training process"
               
               #cut -f2 docfils | join -v1 - rel.$TOPIC.fil > $TOPIC.allNoRel.docfils
               #cut -f1 $TOPIC.allNoRel.docfils | sort -R | head -$R | sort | join - ../svm.fil | sed -e's/[^ ]*/-1/' >> trainset
               cut -f2 docfils | sort -R | head -$R | sort | join - ../$CORP.svm.fil | sed -e's/[^ ]*/-1/' >> trainset
               cat new[0-9][0-9].$TOPIC > seed
               #cut -f2 docfil | join - $TOPIC.clusteringJudged.doc.sorted | cut -d' ' -f2 >> seed
               cat seed | sort | join - rel.$TOPIC.fil | sed -e 's/^/1 /' > x
               #cat seed | sort | join -v1 - rel.$TOPIC.fil | join -v1 - $TOPIC.clusteringNotRel.doc.sorted | sort -R | head -50000 | sed -e 's/^/-1 /' >> x
               cat seed | sort | join -v1 - rel.$TOPIC.fil | sort -R | head -50000 | sed -e 's/^/-1 /' >> x
               sort -k2 x | join -12 - ../$CORP.svm.fil | cut -d' ' -f2- | sort -n >> trainset
               #Calculate relevant documents prevalence rate in the traning set
                
                
                                       
               
               RELTRAINDOC=`grep -E "^1\b" trainset | wc -l`
               NOTRELTRAINDOC=`grep -E "^-1\b" trainset | wc -l`
               PREVALENCERATE=`echo "scale=4; $RELTRAINDOC / ($RELTRAINDOC + $NOTRELTRAINDOC)" | bc`
               echo $RELTRAINDOC $NOTRELTRAINDOC $PREVALENCERATE >> prevalence.rate
               

               $SOFIA --learner_type logreg-pegasos --loop_type roc --lambda $LAMBDA --iterations 200000 --training_file trainset --dimensionality 9300000 --model_out svm_model &>/dev/null
               #/home/user/svmlight/svm_learn trainset

               RES=$?
               echo $RES
               if [ "$RES" -eq "0" ] ; then
                  for z in svm.test.* ; do
                     $SOFIA --test_file $z --dimensionality 9300000 --model_in svm_model --results_file pout.$z &>/dev/null
                     #/home/user/svmlight/svm_classify $z svm_model pout.$z
                  done
               else
                  rm -f pout.svm.test.*
                  cut -f2 docfils | sort -R | cat -n | sort -k2 | sed -e 's/ */-/' > pout.svm.test.1
               fi
#                cut -f1 pout.svm.test.* | ../fixnum | cat -n | join -o2.2,1.2 -t$'\t' - docfils > inlr.out
#                sort seed | join -v2 - inlr.out | sort -rn -k2 | cut -d' ' -f1 > new$N.$TOPIC
#                cat new[0-9][0-9].$TOPIC > x
#                if [ "$N" != "99" ] ; then
#                   head -$L new$N.$TOPIC > y ; mv y new$N.$TOPIC
#                fi


#SCAL 
                echo "starting SCAL process $N"

               cat new[0-9][0-9].$TOPIC > seed.$TOPIC
               cut -f1 pout.svm.test.* | ../fixnum | cat -n | join -o2.2,1.2 -t$'\t' - docfils | sort -k1 -n  > inlr.out.$N.$TOPIC
               echo "inl.out size =`wc -l < inlr.out.$N.$TOPIC` docfils  `wc -l <docfils `"
               
               sort -n seed.$TOPIC > temp
               cat temp | join  -v2 - inlr.out.$N.$TOPIC -2 1 | shuf |  sort -k 2 -r -g -s  > ranking.$N.$TOPIC
               
               #para teste
               head -10 ranking.$N.$TOPIC 
              
              
               echo "ranking size `wc -l < ranking.$N.$TOPIC`"
               
               cat ranking.$N.$TOPIC | cut -d' ' -f1 > new$N.$TOPIC
               cp new$N.$TOPIC U$N
               cat new[0-9][0-9].$TOPIC > x
               if [ "$N" != "99" ] ; then
                  head -$L new$N.$TOPIC > y ;                    
                  mv y new$N.$TOPIC
               fi

                
                if [ "$L" -le 30 ]
                then
                        b=`echo $L`       
                else
                        b=`echo 30`       
                fi 
  
                shuf -n $b  new$N.$TOPIC > sub_new$N.$TOPIC
                
                #computa os relevants
                
               
                    
############################################# 
           
               python2.7 ../doJudgementMain.py --topic=$TOPIC --judgefile=../judgement/qrels.$JUDGECLASS.list --input=sub_new$N.$TOPIC --output=rel.$TOPIC.$N.Judged.doc.list --record=$TOPIC.record.list

               cat rel.$TOPIC.$N.Judged.doc.list >> rel.$TOPIC.fil
               cat rel.$TOPIC.$N.Judged.doc.list > rel.$TOPIC.Judged.doc.list

               RELFINDDOC=`wc -l < rel.$TOPIC.$N.Judged.doc.list`
               
               
               aux=$((($RELFINDDOC*$L)/$b))    
               Rel=$(($Rel+$aux*1000))
               
               
               RELRATE=`echo "scale=4; $RELFINDDOC / $L" | bc`
               CURRENTREL=`wc -l < rel.$TOPIC.fil`
               echo $RELFINDDOC $L $RELRATE $CURRENTREL >> rel.rate
               echo "rel $RELFINDDOC L $L  REL $RELRATE CURRENTRE $CURRENTRE total relevants `wc -l < rel.$TOPIC.fil` "
               sort rel.$TOPIC.fil | sed -e 's/$/ 1/' > prel.$TOPIC
               
               cut -d' ' -f1 prel.$TOPIC > rel.$TOPIC.fil

               NDUN=$((NDUN+L))
               L=$((L+(L+9)/10))
            fi
            done
         done
         # cp judge.effort.$TOPIC."$PURPOSE".dump ../result/dump/"$PURPOSE"/"$CORP"/judge.effort.$TOPIC."$PURPOSE".dump

       #  rm -rf svm.test.*
         popd

        # mv $TOPIC result/"$PURPOSE"/"$CORP"/$TOPIC
      #   rm $TOPIC.seed.doc

      done < "judgement/$CORP.topic.stemming.txt"
      #rm -rf "$CORP".svm.fil
      #rm "$CORP".df

      #rm N

      #Generate LSI from tfdf
      #python clustering/doLSI.py --input=tfdf_oldreut --output=LSIVector/"$CORP".lsi.dump --mapping=LSIVector/"$CORP".mapping.dump --latent=200 --choice=entropy --normalization=yes

   done
