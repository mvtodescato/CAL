#!/bin/bash


PURPOSE=baseline
JUDGECLASS="oldreut"

CORPLIST=("oldreut")
SOFIA="/home/jean/Documents/tar-toolkit-core-copy/sofia-ml-read-only/sofia-ml"

: '
    Percorre cada item listado em
    CORPLIST.
'
for CORP in "${CORPLIST[@]}"; do

    pushd Corpus # pushd <dir> é semelhante à cd <dir>

    ./dofast4 "$CORP"

    cp "$CORP".df ../"$CORP".df

    cp "$CORP".svm.fil ../"$CORP".svm.fil

    popd # popd é semelhante ao cd ..

    : '
        IFS significa Internal Field Separator (separador interno de campos) e
            é utilizado pelo bash para dizer como fazer a separação de palavras,
            ou seja, como identificar palavras (o valor padrã é um espaço em branco " ").

        read <line> permite ler um arquivo linha por linha. -r diz ao bash para não
            permitir que barra invevrtida escape caracteres. IFS="" faz com que
            espaços em brancos no começo e no fim não sejam removidos durante a leitura.

        Sintaxe para leitura de arquivos linha por linha:
            while IFS="" rean -r line; do
                <comandos que irão tratar $line>
            done < input_file
    '
    while IFS='' read -r line || [[ -n $line ]]; do

        : '
        read -ra TEXT atribui o conteúdo de $line a um array chamado TEXT. Neste caso,
            os elementos da linha são separados em cada ":" e cada um vira um elemento do array.
        '
        IFS=':' read -ra TEXT <<< "$line"

        TOPIC="${TEXT[0]}"
        QUERY="${TEXT[1]}"

        echo "$TOPIC"
        echo "$QUERY"

        rm -rf result/"$PURPOSE"/"$CORP"/"$TOPIC"/
        mkdir -p result/"$PURPOSE"/"$CORP"/
        mkdir -p result/dump/"$PURPOSE"/"$CORP"/

        rm -rf $TOPIC
        mkdir $TOPIC


        : '
            wc -l mostra o número de linhas de um arquivo. Assim, "N" armazena
                o número de linhas do arquivo "$CORP.svm.fil".
        '
        echo `wc -l < "$CORP".svm.fil` > N

        pushd $TOPIC

        echo "$QUERY" > "$TOPIC".seed.doc

        : '
            cut é utilizado para selecionar (cortar) partes de um arquivo.

                -f permite selecionar com base em um campo (neste caso
                o primeiro: -f1 ou -f 1).

                -d especifica o delimitador, utilizado para separar os campos.

                cut -d " " -f 1 forma campos dividindo cada linha nos espaços em
                branco e seleciona a primeira coluna.

            sed é um editor (não interativo) que permite, dentre outras coisas,
                realizar substituições em uma entrada de texto.

                -e <script> diz para executar <script>. Neste caso não é necessário
                pois há apenas um script/comando.

                em "s/.*/& &/", s informa que será executada uma substituição no texto
                de entrada. .* é o pattern que deve ser substituido pelo pattern & &.
                    .* representa qualquer caracter exceto quebra de linha, ou seja,
                    permite selecionar uma linha inteira.

                    & representa toda a string encontrada.

                então sed "s/.*/& &/" duplica o conteúdo de cada linha
                de um arquivo.
        '

        cut -d' ' -f1 ../$CORP.svm.fil | sed -e 's/.*/& &/' > docfil
        cut -d' ' -f1 docfil | cat -n > docfils


        touch rel.$TOPIC.fil
        touch prel.$TOPIC

        rm -rf prevalence.rate
        touch prevalence.rate

        rm -rf rel.rate
        touch rel.rate

        ########################################################################
        ########################################################################
        ########################################################################
        ########################################################################
        ########################################################################
        ########################################################################
        ########################################################################
        ########################################################################
        ########################################################################
        ########################################################################
        # STOPED HERE

        rm -f new[0-9][0-9].$TOPIC tail[0-9][0-9].$TOPIC self*.$TOPIC gold*.$TOPIC
        touch new00.$TOPIC


        NDOCS=`cat docfils | wc -l`
        NDUN=0
        L=1
        R=100
        export LAMBDA=0.0001

        cp $TOPIC.seed.doc ../$TOPIC.seed.doc

        popd

        ./dofeaturesseed4 $TOPIC.seed.doc $TOPIC $CORP

        pushd $TOPIC

        sed -e 's/[^ ]*/0/' ../$CORP.svm.fil | ../dosplit
        sed -e 's/[^ ]*/1/' svm.$TOPIC.seed.doc.fil > $TOPIC.synthetic.seed


        for x in 0 1 2 3 4 5 6 7 8 9 ; do

            for y in 0 1 2 3 4 5 6 7 8 9 ; do

                if [ $NDUN -lt $NDOCS ] ; then
                    export N=$x$y
                    cp $TOPIC.synthetic.seed trainset

                    cut -f2 docfils | sort -R | head -$R | sort | join - ../$CORP.svm.fil | sed -e's/[^ ]*/-1/' >> trainset

                    cat new[0-9][0-9].$TOPIC > seed
                    cat seed | sort | join - rel.$TOPIC.fil | sed -e 's/^/1 /' > x
                    cat seed | sort | join -v1 - rel.$TOPIC.fil | sort -R | head -50000 | sed -e 's/^/-1 /' >> x

                    sort -k2 x | join -12 - ../$CORP.svm.fil | cut -d' ' -f2- | sort -n >> trainset

                    # Calculate relevant documents prevalence rate in the traning set

                    RELTRAINDOC=`grep -E "^1\b" trainset | wc -l`
                    NOTRELTRAINDOC=`grep -E "^-1\b" trainset | wc -l`
                    PREVALENCERATE=`echo "scale=4; $RELTRAINDOC / ($RELTRAINDOC + $NOTRELTRAINDOC)" | bc`
                    echo $RELTRAINDOC $NOTRELTRAINDOC $PREVALENCERATE >> prevalence.rate


                    $SOFIA --learner_type logreg-pegasos --loop_type roc --lambda $LAMBDA --iterations 200000 --training_file trainset --dimensionality 9300000 --model_out svm_model

                    RES=$?
                    echo $RES

                    if [ "$RES" -eq "0" ] ; then
                        for z in svm.test.* ; do
                            $SOFIA --test_file $z --dimensionality 9300000 --model_in svm_model --results_file pout.$z
                        done

                    else
                        rm -f pout.svm.test.*
                        cut -f2 docfils | sort -R | cat -n | sort -k2 | sed -e 's/ */-/' > pout.svm.test.1
                    fi

                    cut -f1 pout.svm.test.* | ../fixnum | cat -n | join -o2.2,1.2 -t$'\t' - docfils > inlr.out
                    sort seed | join -v2 - inlr.out | sort -rn -k2 | cut -d' ' -f1 > new$N.$TOPIC
                    cat new[0-9][0-9].$TOPIC > x

                    if [ "$N" != "99" ] ; then
                        head -$L new$N.$TOPIC > y ; mv y new$N.$TOPIC
                    fi

                    python ../doJudgementMain.py --topic=$TOPIC --judgefile=../judgement/qrels.$JUDGECLASS.list --input=new$N.$TOPIC --output=rel.$TOPIC.Judged.doc.list --record=$TOPIC.record.list

                    cat rel.$TOPIC.Judged.doc.list >> rel.$TOPIC.fil
                    cat rel.$TOPIC.Judged.doc.list > rel.$TOPIC.$N.Judged.doc.list

                    RELFINDDOC=`wc -l < rel.$TOPIC.Judged.doc.list`
                    RELRATE=`echo "scale=4; $RELFINDDOC / $L" | bc`
                    CURRENTREL=`wc -l < rel.$TOPIC.fil`
                    echo $RELFINDDOC $L $RELRATE $CURRENTREL >> rel.rate

                    sort rel.$TOPIC.fil | sed -e 's/$/ 1/' > prel.$TOPIC

                    cut -d' ' -f1 prel.$TOPIC > rel.$TOPIC.fil

                    NDUN=$((NDUN+L))
                    L=$((L+(L+9)/10))
                fi
            done
        done

        rm -rf svm.test.*
        popd

        mv $TOPIC result/"$PURPOSE"/"$CORP"/$TOPIC
        rm $TOPIC.seed.doc

    done < "judgement/$CORP.topic.stemming.txt"

    rm -rf "$CORP".svm.fil
    rm "$CORP".df

    rm N

    # Generate LSI from tfdf
    # python clustering/doLSI.py --input=tfdf_oldreut --output=LSIVector/"$CORP".lsi.dump --mapping=LSIVector/"$CORP".mapping.dump --latent=200 --choice=entropy --normalization=yes

done
